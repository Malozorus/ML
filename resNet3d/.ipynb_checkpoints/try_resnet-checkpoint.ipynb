{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import pytorch_lightning\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_video\n",
    "import cv2\n",
    "from torch.utils.data import random_split\n",
    "import random\n",
    "from torch.utils.data import Subset\n",
    "from torchvision.io import VideoReader\n",
    "\n",
    "import cv2\n",
    "\n",
    "def load_video_with_opencv(video_path):\n",
    "    vid = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = vid.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Convertir en niveaux de gris (même si c'est déjà le cas)\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        frames.append(torch.tensor(gray_frame))\n",
    "    vid.release()\n",
    "    video_tensor = torch.stack(frames)\n",
    "    return video_tensor.unsqueeze(1)  # Ajoute un canal\n",
    "\n",
    "\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - root_dir (string): Dossier avec toutes les vidéos.\n",
    "        - transform (callable, optional): Transformation optionnelle à appliquer\n",
    "            sur une vidéo.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.classes = os.listdir(root_dir)\n",
    "        self.classes.sort()\n",
    "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "\n",
    "        self.video_list = []\n",
    "        for cls_name in self.classes:\n",
    "            cls_dir = os.path.join(root_dir, cls_name)\n",
    "            for video_name in os.listdir(cls_dir):\n",
    "                self.video_list.append((os.path.join(cls_dir, video_name), self.class_to_idx[cls_name]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path, label = self.video_list[idx]\n",
    "        \n",
    "        # Charger uniquement le flux vidéo (et ignorer l'audio et les timestamps)\n",
    "        video = load_video_with_opencv(video_path)\n",
    "\n",
    "        # S'assurer que la vidéo est sur le GPU si disponible\n",
    "        if torch.cuda.is_available():\n",
    "            video = video.to(\"cuda\")\n",
    "        \n",
    "        if self.transform:\n",
    "            video = self.transform(video)\n",
    "\n",
    "        return video, label\n",
    "    \n",
    "\n",
    "class VideoDataModule(pytorch_lightning.LightningDataModule):\n",
    "\n",
    "    # Configuration du Dataset\n",
    "    _DATA_PATH = \"C:/Users/mcouv/Work/machine-learning/ML_tuto/classes\"\n",
    "    _BATCH_SIZE = 4\n",
    "    _NUM_WORKERS = 0  # Nombre de processus parallèles récupérant les données\n",
    "\n",
    "    def __init__(self, transform=None):\n",
    "        super(VideoDataModule, self).__init__()\n",
    "        self.transform = transform\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Initialiser le dataset avec la transformation\n",
    "        video_dataset = VideoDataset(root_dir=self._DATA_PATH, transform=self.transform)\n",
    "\n",
    "        # La logique de séparation train/test est déjà définie dans le code donné, donc on la réutilise ici.\n",
    "        train_indices = []\n",
    "        test_indices = []\n",
    "\n",
    "        # Pour chaque classe, récupérez les indices de ses vidéos.\n",
    "        for cls_name in video_dataset.classes:\n",
    "            cls_indices = [i for i, (path, label) in enumerate(video_dataset.video_list) if label == video_dataset.class_to_idx[cls_name]]\n",
    "            \n",
    "            # Mélangez ces indices.\n",
    "            random.shuffle(cls_indices)\n",
    "            \n",
    "            # Séparez-les en fonction des ratios d'entraînement et de test.\n",
    "            cls_train_size = int(0.8 * len(cls_indices))\n",
    "            train_indices.extend(cls_indices[:cls_train_size])\n",
    "            test_indices.extend(cls_indices[cls_train_size:])\n",
    "\n",
    "        # Créez des sous-ensembles d'entraînement et de test en utilisant ces indices.\n",
    "        self.train_dataset = Subset(video_dataset, train_indices)\n",
    "        self.test_dataset = Subset(video_dataset, test_indices)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        \"\"\"\n",
    "        Créer le DataLoader pour le partition d'entraînement\n",
    "        \"\"\"\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self._BATCH_SIZE,\n",
    "            num_workers=self._NUM_WORKERS,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        \"\"\"\n",
    "        Créer le DataLoader pour le partition de validation\n",
    "        \"\"\"\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self._BATCH_SIZE,\n",
    "            num_workers=self._NUM_WORKERS,\n",
    "            shuffle=False,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import cv2\n",
    "from torch.utils.data import random_split\n",
    "import random\n",
    "from torch.utils.data import Subset\n",
    "import numpy as np\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def uniform_frame_sampling(video_tensor, target_frames=90):\n",
    "    \"\"\"\n",
    "    Sélectionne un nombre uniformément distribué de frames pour que toutes les vidéos aient la même taille.\n",
    "\n",
    "    Args:\n",
    "        video_tensor (torch.Tensor): La vidéo originale de forme (T, C, H, W)\n",
    "        target_frames (int): Nombre de frames cibles à obtenir\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Vidéo échantillonnée de forme (target_frames, C, H, W)\n",
    "    \"\"\"\n",
    "\n",
    "    # Obtenir le nombre actuel de frames\n",
    "    current_frames = video_tensor.shape[0]\n",
    "\n",
    "    # Si la vidéo actuelle a exactement le nombre cible de frames, la renvoyer telle quelle\n",
    "    if current_frames == target_frames:\n",
    "        return video_tensor\n",
    "\n",
    "    # Calculer les indices des frames à échantillonner\n",
    "    indices = torch.linspace(0, current_frames - 1, target_frames).long()\n",
    "\n",
    "    return video_tensor[indices]\n",
    "\n",
    "# Pour l'utiliser dans le VideoDataset :\n",
    "\n",
    "\n",
    "def video_resize_and_sample(video_tensor, size=(8, 16), T=90):\n",
    "    # Resize\n",
    "    transform = transforms.Resize(size, antialias=False)\n",
    "\n",
    "    resized_video = [transform(frame) for frame in video_tensor]\n",
    "    resized_video_tensor = torch.stack(resized_video)\n",
    "    \n",
    "    # Uniform sampling\n",
    "    sampled_video_tensor = uniform_frame_sampling(resized_video_tensor, T)\n",
    "    \n",
    "    # Normalisation entre [-1, 1]\n",
    "    normalized_tensor = (sampled_video_tensor / 127.5) - 1.0\n",
    "\n",
    "    return normalized_tensor.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille de l'ensemble d'entraînement: 227\n",
      "Taille de l'ensemble de validation: 57\n",
      "Forme d'un batch de vidéos: (4, 90, 1, 8, 16)\n",
      "Forme d'un batch de labels: (4,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialisation du VideoDataModule avec la transformation désirée\n",
    "transform = video_resize_and_sample\n",
    "video_data_module = VideoDataModule(transform=transform)\n",
    "video_data_module.setup()\n",
    "\n",
    "# Obtenir les chargeurs de données :\n",
    "train_loader = video_data_module.train_dataloader()\n",
    "val_loader = video_data_module.val_dataloader()\n",
    "\n",
    "# Afficher des informations générales\n",
    "print(f\"Taille de l'ensemble d'entraînement: {len(train_loader.dataset)}\")\n",
    "print(f\"Taille de l'ensemble de validation: {len(val_loader.dataset)}\")\n",
    "\n",
    "# Obtenir un batch de données et afficher ses dimensions\n",
    "videos_gpu, labels_gpu = next(iter(train_loader))\n",
    "videos = videos_gpu.cpu().numpy()\n",
    "labels = labels_gpu.cpu().numpy()\n",
    "\n",
    "print(f\"Forme d'un batch de vidéos: {videos.shape}\")  # devrait être de la forme (batch_size, T, C, H, W)\n",
    "print(f\"Forme d'un batch de labels: {labels.shape}\") \n",
    "\n",
    "# Afficher un extrait d'une frame\n",
    "plt.imshow(videos[0, 0, 0], cmap='hot')\n",
    "plt.title(f\"Label: {labels[0]}\")\n",
    "plt.show()\n",
    "\n",
    "# Compter le nombre de vidéos par classe dans l'ensemble d'entraînement\n",
    "# class_counts = {}\n",
    "# for _, label in train_loader.dataset:\n",
    "#     class_name = video_data_module.train_dataset.dataset.classes[label]\n",
    "#     class_counts[class_name] = class_counts.get(class_name, 0) + 1\n",
    "\n",
    "# print(\"Nombre de vidéos par classe dans l'ensemble d'entraînement:\")\n",
    "# for cls, count in class_counts.items():\n",
    "#     print(f\"{cls}: {count}\")\n",
    "\n",
    "# Si vous souhaitez afficher des statistiques supplémentaires, vous pouvez étendre ce script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorchvideo.models.resnet\n",
    "import os\n",
    "import pytorch_lightning\n",
    "import pytorchvideo.data\n",
    "import torch.utils.data\n",
    "\n",
    "def make_kinetics_resnet():\n",
    "  return pytorchvideo.models.resnet.create_resnet(\n",
    "    input_channel=1,\n",
    "    model_depth=50,  # Choix arbitraire, pourrait être modifié selon les besoins.\n",
    "    model_num_class=2,  # Classification binaire.\n",
    "    stem_conv_kernel_size=(3, 3, 3),\n",
    "    stem_conv_stride=(1, 1, 1),\n",
    "    stem_pool_kernel_size=(1, 2, 2),\n",
    "    stem_pool_stride=(1, 2, 2),\n",
    "    head_pool_kernel_size=(4, 2, 2),\n",
    ")\n",
    "\n",
    "\n",
    "class VideoClassificationLightningModule(pytorch_lightning.LightningModule):\n",
    "  def __init__(self):\n",
    "      super().__init__()\n",
    "      self.model = SimpleVideoNet().to(\"cuda\")\n",
    "\n",
    "  def forward(self, x):\n",
    "      return self.model(x)\n",
    "\n",
    "  def training_step(self, batch, batch_idx):\n",
    "      # The model expects a video tensor of shape (B, C, T, H, W), which is the\n",
    "      # format provided by the dataset\n",
    "      y_hat = self.model(batch[0].permute(0, 2, 1, 3, 4))\n",
    "\n",
    "      # Compute cross entropy loss, loss.backwards will be called behind the scenes\n",
    "      # by PyTorchLightning after being returned from this method.\n",
    "      loss = F.cross_entropy(y_hat, batch[1])\n",
    "\n",
    "      # Log the train loss to Tensorboard\n",
    "      self.log(\"train_loss\", loss.item())\n",
    "\n",
    "      return loss\n",
    "\n",
    "  def validation_step(self, batch, batch_idx):\n",
    "    y_hat = self.model(batch[0].permute(0, 2, 1, 3, 4))\n",
    "    loss = F.cross_entropy(y_hat, batch[1])\n",
    "    self.log(\"val_loss\", loss)\n",
    "    return loss\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "      \"\"\"\n",
    "      Setup the Adam optimizer. Note, that this function also can return a lr scheduler, which is\n",
    "      usually useful for training video models.\n",
    "      \"\"\"\n",
    "      return torch.optim.Adam(self.parameters(), lr=1e-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class VideoClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VideoClassifier, self).__init__()\n",
    "        \n",
    "        # La couche d'entrée attend des vidéos de la forme (C, T, H, W) = (1, 90, 8, 16)\n",
    "        self.conv_layer1 = nn.Sequential(\n",
    "            nn.R2(1, 32, kernel_size=(3, 3, 3), padding=1),  # 3D convolution\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2))\n",
    "        )\n",
    "        \n",
    "        self.conv_layer2 = nn.Sequential(\n",
    "            nn.Conv3d(32, 64, kernel_size=(3, 3, 3), padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2))\n",
    "        )\n",
    "        \n",
    "        # Ici, on aplatit la sortie pour la connecter à des couches entièrement connectées\n",
    "        self.fc1 = nn.Linear(64 * 22 * 2 * 4, 512)  # La taille dépend de la sortie du dernier MaxPool3d\n",
    "        self.fc2 = nn.Linear(512, 1)  # 1 neurone en sortie pour une classification binaire\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer1(x)\n",
    "        x = self.conv_layer2(x)\n",
    "        x = x.view(x.size(0), -1)  # flatten\n",
    "        x = nn.LeakyReLU()(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    classification_module = VideoClassificationLightningModule()\n",
    "    video_data_module = VideoDataModule(transform=transform)\n",
    "    video_data_module.setup()\n",
    "    trainer = pytorch_lightning.Trainer(max_epochs=10, )\n",
    "    trainer.fit(classification_module, video_data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVideoNet(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(SimpleVideoNet, self).__init__()\n",
    "\n",
    "        # Stem\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(3, 3, 3), padding=(3, 3, 3)), # Convolutions 3D\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2)) # Pooling 3D\n",
    "        )\n",
    "\n",
    "        # Backbone - Simple Conv Layers (you can replace with ResBlocks or MobileNet blocks for better performance)\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
    "        )\n",
    "\n",
    "        # Head\n",
    "        self.head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool3d((1, 1, 1)), # Pooling temporel + spatial\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.backbone(x)\n",
    "        x = self.head(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 2])\n"
     ]
    }
   ],
   "source": [
    "# Création du modèle\n",
    "model = SimpleVideoNet()\n",
    "\n",
    "# Simulation d'un batch de données d'entrée\n",
    "input_tensor = torch.randn(8, 1, 90, 8, 16)\n",
    "\n",
    "# Passage du batch à travers le modèle\n",
    "output = model(input_tensor)\n",
    "\n",
    "# Affichage de la sortie\n",
    "print(output.shape)  # Devrait afficher torch.Size([8, 2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\mcouv\\miniconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3070 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type           | Params\n",
      "-----------------------------------------\n",
      "0 | model | SimpleVideoNet | 56.4 K\n",
      "-----------------------------------------\n",
      "56.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "56.4 K    Total params\n",
      "0.226     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mcouv\\miniconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mcouv\\miniconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  26%|████████████████████████▏                                                                   | 15/57 [00:51<02:25,  3.46s/it, v_num=4]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mcouv\\miniconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:52: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'VideoClassificationLightningModule' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pytorch_lightning\u001b[38;5;241m.\u001b[39mTrainer(max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, log_every_n_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      5\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(classification_module, video_data_module)\n\u001b[1;32m----> 6\u001b[0m \u001b[43mclassification_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'VideoClassificationLightningModule' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "torch.set_float32_matmul_precision('medium')\n",
    "classification_module = VideoClassificationLightningModule()\n",
    "video_data_module = VideoDataModule(transform=transform)\n",
    "video_data_module.setup()\n",
    "trainer = pytorch_lightning.Trainer(max_epochs=10, log_every_n_steps=1)\n",
    "trainer.fit(classification_module, video_data_module)\n",
    "classification_module.model.save(\"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classification_module' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m classification_module\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39mmodel.pth\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'classification_module' is not defined"
     ]
    }
   ],
   "source": [
    "classification_module.save(\"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
