{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import pytorch_lightning\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_video\n",
    "import cv2\n",
    "from torch.utils.data import random_split\n",
    "import random\n",
    "from torch.utils.data import Subset\n",
    "from torchvision.io import VideoReader\n",
    "\n",
    "import cv2\n",
    "\n",
    "def load_video_with_opencv(video_path):\n",
    "    vid = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = vid.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Convertir en niveaux de gris (même si c'est déjà le cas)\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        frames.append(torch.tensor(gray_frame))\n",
    "    vid.release()\n",
    "    video_tensor = torch.stack(frames)\n",
    "    return video_tensor.unsqueeze(1)  # Ajoute un canal\n",
    "\n",
    "\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - root_dir (string): Dossier avec toutes les vidéos.\n",
    "        - transform (callable, optional): Transformation optionnelle à appliquer\n",
    "            sur une vidéo.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.classes = os.listdir(root_dir)\n",
    "        self.classes.sort()\n",
    "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "\n",
    "        self.video_list = []\n",
    "        for cls_name in self.classes:\n",
    "            cls_dir = os.path.join(root_dir, cls_name)\n",
    "            for video_name in os.listdir(cls_dir):\n",
    "                self.video_list.append((os.path.join(cls_dir, video_name), self.class_to_idx[cls_name]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path, label = self.video_list[idx]\n",
    "        \n",
    "        # Charger uniquement le flux vidéo (et ignorer l'audio et les timestamps)\n",
    "        video = load_video_with_opencv(video_path)\n",
    "\n",
    "        # S'assurer que la vidéo est sur le GPU si disponible\n",
    "        if torch.cuda.is_available():\n",
    "            video = video.to(\"cuda:0\")\n",
    "        \n",
    "        if self.transform:\n",
    "            video = self.transform(video)\n",
    "\n",
    "        return video, label\n",
    "    \n",
    "\n",
    "class VideoDataModule(pytorch_lightning.LightningDataModule):\n",
    "\n",
    "    # Configuration du Dataset\n",
    "    _DATA_PATH = \"C:/Users/mcouv/Work/machine-learning/ML_tuto/classes\"\n",
    "    _BATCH_SIZE = 4\n",
    "    _NUM_WORKERS = 0  # Nombre de processus parallèles récupérant les données\n",
    "\n",
    "    def __init__(self, transform=None):\n",
    "        super(VideoDataModule, self).__init__()\n",
    "        self.transform = transform\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Initialiser le dataset avec la transformation\n",
    "        video_dataset = VideoDataset(root_dir=self._DATA_PATH, transform=self.transform)\n",
    "\n",
    "        # La logique de séparation train/test est déjà définie dans le code donné, donc on la réutilise ici.\n",
    "        train_indices = []\n",
    "        test_indices = []\n",
    "\n",
    "        # Pour chaque classe, récupérez les indices de ses vidéos.\n",
    "        for cls_name in video_dataset.classes:\n",
    "            cls_indices = [i for i, (path, label) in enumerate(video_dataset.video_list) if label == video_dataset.class_to_idx[cls_name]]\n",
    "            \n",
    "            # Mélangez ces indices.\n",
    "            random.shuffle(cls_indices)\n",
    "            \n",
    "            # Séparez-les en fonction des ratios d'entraînement et de test.\n",
    "            cls_train_size = int(0.8 * len(cls_indices))\n",
    "            train_indices.extend(cls_indices[:cls_train_size])\n",
    "            test_indices.extend(cls_indices[cls_train_size:])\n",
    "\n",
    "        # Créez des sous-ensembles d'entraînement et de test en utilisant ces indices.\n",
    "        self.train_dataset = Subset(video_dataset, train_indices)\n",
    "        self.test_dataset = Subset(video_dataset, test_indices)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        \"\"\"\n",
    "        Créer le DataLoader pour le partition d'entraînement\n",
    "        \"\"\"\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self._BATCH_SIZE,\n",
    "            num_workers=self._NUM_WORKERS,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        \"\"\"\n",
    "        Créer le DataLoader pour le partition de validation\n",
    "        \"\"\"\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self._BATCH_SIZE,\n",
    "            num_workers=self._NUM_WORKERS,\n",
    "            shuffle=False,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import cv2\n",
    "from torch.utils.data import random_split\n",
    "import random\n",
    "from torch.utils.data import Subset\n",
    "import numpy as np\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def uniform_frame_sampling(video_tensor, target_frames=90):\n",
    "    \"\"\"\n",
    "    Sélectionne un nombre uniformément distribué de frames pour que toutes les vidéos aient la même taille.\n",
    "\n",
    "    Args:\n",
    "        video_tensor (torch.Tensor): La vidéo originale de forme (T, C, H, W)\n",
    "        target_frames (int): Nombre de frames cibles à obtenir\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Vidéo échantillonnée de forme (target_frames, C, H, W)\n",
    "    \"\"\"\n",
    "\n",
    "    # Obtenir le nombre actuel de frames\n",
    "    current_frames = video_tensor.shape[0]\n",
    "\n",
    "    # Si la vidéo actuelle a exactement le nombre cible de frames, la renvoyer telle quelle\n",
    "    if current_frames == target_frames:\n",
    "        return video_tensor\n",
    "\n",
    "    # Calculer les indices des frames à échantillonner\n",
    "    indices = torch.linspace(0, current_frames - 1, target_frames).long()\n",
    "\n",
    "    return video_tensor[indices]\n",
    "\n",
    "# Pour l'utiliser dans le VideoDataset :\n",
    "\n",
    "\n",
    "def video_resize_and_sample(video_tensor, size=(8, 16), T=90):\n",
    "    # Resize\n",
    "    transform = transforms.Resize(size, antialias=False)\n",
    "\n",
    "    resized_video = [transform(frame) for frame in video_tensor]\n",
    "    resized_video_tensor = torch.stack(resized_video)\n",
    "    \n",
    "    # Uniform sampling\n",
    "    sampled_video_tensor = uniform_frame_sampling(resized_video_tensor, T)\n",
    "    \n",
    "    # Normalisation entre [-1, 1]\n",
    "    normalized_tensor = (sampled_video_tensor / 127.5) - 1.0\n",
    "\n",
    "    return normalized_tensor.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille de l'ensemble d'entraînement: 227\n",
      "Taille de l'ensemble de validation: 57\n",
      "Forme d'un batch de vidéos: (4, 90, 1, 8, 16)\n",
      "Forme d'un batch de labels: (4,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialisation du VideoDataModule avec la transformation désirée\n",
    "transform = video_resize_and_sample\n",
    "video_data_module = VideoDataModule(transform=transform)\n",
    "video_data_module.setup()\n",
    "\n",
    "# Obtenir les chargeurs de données :\n",
    "train_loader = video_data_module.train_dataloader()\n",
    "val_loader = video_data_module.val_dataloader()\n",
    "\n",
    "# Afficher des informations générales\n",
    "print(f\"Taille de l'ensemble d'entraînement: {len(train_loader.dataset)}\")\n",
    "print(f\"Taille de l'ensemble de validation: {len(val_loader.dataset)}\")\n",
    "\n",
    "# Obtenir un batch de données et afficher ses dimensions\n",
    "videos_gpu, labels_gpu = next(iter(train_loader))\n",
    "videos = videos_gpu.cpu().numpy()\n",
    "labels = labels_gpu.cpu().numpy()\n",
    "\n",
    "print(f\"Forme d'un batch de vidéos: {videos.shape}\")  # devrait être de la forme (batch_size, T, C, H, W)\n",
    "print(f\"Forme d'un batch de labels: {labels.shape}\") \n",
    "\n",
    "# Afficher un extrait d'une frame\n",
    "plt.imshow(videos[0, 0, 0], cmap='hot')\n",
    "plt.title(f\"Label: {labels[0]}\")\n",
    "plt.show()\n",
    "\n",
    "# Compter le nombre de vidéos par classe dans l'ensemble d'entraînement\n",
    "# class_counts = {}\n",
    "# for _, label in train_loader.dataset:\n",
    "#     class_name = video_data_module.train_dataset.dataset.classes[label]\n",
    "#     class_counts[class_name] = class_counts.get(class_name, 0) + 1\n",
    "\n",
    "# print(\"Nombre de vidéos par classe dans l'ensemble d'entraînement:\")\n",
    "# for cls, count in class_counts.items():\n",
    "#     print(f\"{cls}: {count}\")\n",
    "\n",
    "# Si vous souhaitez afficher des statistiques supplémentaires, vous pouvez étendre ce script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorchvideo.models.resnet\n",
    "import os\n",
    "import pytorch_lightning\n",
    "import pytorchvideo.data\n",
    "import torch.utils.data\n",
    "\n",
    "def make_kinetics_resnet():\n",
    "  return pytorchvideo.models.resnet.create_resnet(\n",
    "    input_channel=1,\n",
    "    model_depth=50,  # Choix arbitraire, pourrait être modifié selon les besoins.\n",
    "    model_num_class=2,  # Classification binaire.\n",
    "    stem_conv_kernel_size=(3, 3, 3),\n",
    "    stem_conv_stride=(1, 1, 1),\n",
    "    stem_pool_kernel_size=(1, 2, 2),\n",
    "    stem_pool_stride=(1, 2, 2),\n",
    "    head_pool_kernel_size=(4, 2, 2),\n",
    ")\n",
    "\n",
    "\n",
    "class VideoClassificationLightningModule(pytorch_lightning.LightningModule):\n",
    "  def __init__(self):\n",
    "      super().__init__()\n",
    "      self.model = SimpleVideoNet().to(\"cuda:0\")\n",
    "\n",
    "  def forward(self, x):\n",
    "      return self.model(x)\n",
    "\n",
    "  def training_step(self, batch, batch_idx):\n",
    "      # The model expects a video tensor of shape (B, C, T, H, W), which is the\n",
    "      # format provided by the dataset\n",
    "      y_hat = self.model(batch[0].permute(0, 2, 1, 3, 4))\n",
    "\n",
    "      # Compute cross entropy loss, loss.backwards will be called behind the scenes\n",
    "      # by PyTorchLightning after being returned from this method.\n",
    "      loss = F.cross_entropy(y_hat, batch[1])\n",
    "\n",
    "      # Log the train loss to Tensorboard\n",
    "      self.log(\"train_loss\", loss.item())\n",
    "\n",
    "      return loss\n",
    "\n",
    "  def validation_step(self, batch, batch_idx):\n",
    "    y_hat = self.model(batch[0].permute(0, 2, 1, 3, 4))\n",
    "    loss = F.cross_entropy(y_hat, batch[1])\n",
    "    self.log(\"val_loss\", loss)\n",
    "    return loss\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "      \"\"\"\n",
    "      Setup the Adam optimizer. Note, that this function also can return a lr scheduler, which is\n",
    "      usually useful for training video models.\n",
    "      \"\"\"\n",
    "      return torch.optim.Adam(self.parameters(), lr=1e-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class VideoClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VideoClassifier, self).__init__()\n",
    "        \n",
    "        # La couche d'entrée attend des vidéos de la forme (C, T, H, W) = (1, 90, 8, 16)\n",
    "        self.conv_layer1 = nn.Sequential(\n",
    "            nn.R2(1, 32, kernel_size=(3, 3, 3), padding=1),  # 3D convolution\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2))\n",
    "        )\n",
    "        \n",
    "        self.conv_layer2 = nn.Sequential(\n",
    "            nn.Conv3d(32, 64, kernel_size=(3, 3, 3), padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2))\n",
    "        )\n",
    "        \n",
    "        # Ici, on aplatit la sortie pour la connecter à des couches entièrement connectées\n",
    "        self.fc1 = nn.Linear(64 * 22 * 2 * 4, 512)  # La taille dépend de la sortie du dernier MaxPool3d\n",
    "        self.fc2 = nn.Linear(512, 1)  # 1 neurone en sortie pour une classification binaire\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer1(x)\n",
    "        x = self.conv_layer2(x)\n",
    "        x = x.view(x.size(0), -1)  # flatten\n",
    "        x = nn.LeakyReLU()(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    classification_module = VideoClassificationLightningModule()\n",
    "    video_data_module = VideoDataModule(transform=transform)\n",
    "    video_data_module.setup()\n",
    "    trainer = pytorch_lightning.Trainer(max_epochs=10, )\n",
    "    trainer.fit(classification_module, video_data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVideoNet(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(SimpleVideoNet, self).__init__()\n",
    "\n",
    "        # Stem\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(3, 3, 3), padding=(3, 3, 3)), # Convolutions 3D\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2)) # Pooling 3D\n",
    "        )\n",
    "\n",
    "        # Backbone - Simple Conv Layers (you can replace with ResBlocks or MobileNet blocks for better performance)\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
    "        )\n",
    "\n",
    "        # Head\n",
    "        self.head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool3d((1, 1, 1)), # Pooling temporel + spatial\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.backbone(x)\n",
    "        x = self.head(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 2])\n"
     ]
    }
   ],
   "source": [
    "# Création du modèle\n",
    "model = SimpleVideoNet()\n",
    "\n",
    "# Simulation d'un batch de données d'entrée\n",
    "input_tensor = torch.randn(8, 1, 90, 8, 16)\n",
    "\n",
    "# Passage du batch à travers le modèle\n",
    "output = model(input_tensor)\n",
    "\n",
    "# Affichage de la sortie\n",
    "print(output.shape)  # Devrait afficher torch.Size([8, 2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type           | Params\n",
      "-----------------------------------------\n",
      "0 | model | SimpleVideoNet | 56.4 K\n",
      "-----------------------------------------\n",
      "56.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "56.4 K    Total params\n",
      "0.226     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mcouv\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mcouv\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 57/57 [04:00<00:00,  4.22s/it, v_num=5]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 57/57 [04:00<00:00,  4.22s/it, v_num=5]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SimpleVideoNet' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m trainer \u001b[39m=\u001b[39m pytorch_lightning\u001b[39m.\u001b[39mTrainer(max_epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, log_every_n_steps\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m      6\u001b[0m trainer\u001b[39m.\u001b[39mfit(classification_module, video_data_module)\n\u001b[1;32m----> 7\u001b[0m classification_module\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49msave(\u001b[39m\"\u001b[39m\u001b[39mmodel.pth\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1612\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[0;32m   1613\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1614\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1615\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SimpleVideoNet' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "torch.set_float32_matmul_precision('medium')\n",
    "classification_module = VideoClassificationLightningModule()\n",
    "video_data_module = VideoDataModule(transform=transform)\n",
    "video_data_module.setup()\n",
    "trainer = pytorch_lightning.Trainer(max_epochs=10, log_every_n_steps=1)\n",
    "trainer.fit(classification_module, video_data_module)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.save(classification_module.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Le volume dans le lecteur C s'appelle Windows\n",
      " Le num�ro de s�rie du volume est 92DF-69A0\n",
      "\n",
      " R�pertoire de c:\\Users\\mcouv\\Work\\machine-learning\\ML_tuto\\resNet3d\n",
      "\n",
      "03/09/2023  20:44    <DIR>          .\n",
      "03/09/2023  19:11    <DIR>          ..\n",
      "03/09/2023  19:11    <DIR>          .ipynb_checkpoints\n",
      "03/09/2023  13:18                 0 __init__.py\n",
      "03/09/2023  19:07    <DIR>          __pycache__\n",
      "03/09/2023  18:53             5�879 datamodule.py\n",
      "03/09/2023  20:05    <DIR>          lightning_logs\n",
      "03/09/2023  20:48           227�719 model.pth\n",
      "03/09/2023  19:03             2�977 model.py\n",
      "03/09/2023  18:54               717 train.py\n",
      "03/09/2023  20:49            35�617 try_resnet.ipynb\n",
      "03/09/2023  18:45             2�332 Untitled.ipynb\n",
      "               7 fichier(s)          275�241 octets\n",
      "               5 R�p(s)  255�212�912�640 octets libres\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m         _, predicted \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(outputs, \u001b[39m1\u001b[39m)\n\u001b[0;32m     14\u001b[0m         total \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m         correct \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (predicted \u001b[39m==\u001b[39;49m labels)\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     17\u001b[0m accuracy \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m \u001b[39m*\u001b[39m correct \u001b[39m/\u001b[39m total\n\u001b[0;32m     18\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAccuracy of the model on the validation data: \u001b[39m\u001b[39m{\u001b[39;00maccuracy\u001b[39m}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "model = classification_module.model\n",
    "val_dataloader = video_data_module.val_dataloader()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch in val_dataloader:\n",
    "        # Je suppose que vos données sont sous forme (videos, labels)\n",
    "        videos, labels = batch\n",
    "        outputs = model(videos.permute(0, 2, 1, 3, 4))\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Accuracy of the model on the validation data: {accuracy}%\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
