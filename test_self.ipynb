{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the recorded domalys data with pandas and excel file saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path to the excel file : \n",
    "path = \"C:/Users/mcouv/Work/machine-learning/ia_fall_confirmation\"\n",
    "records = pd.read_excel(path + \"/record_data_thermal_all.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>test_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>thermal_top</th>\n",
       "      <th>thermal_bot</th>\n",
       "      <th>fall</th>\n",
       "      <th>temperature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>16:22:55</td>\n",
       "      <td>[[1984 1901 1873 1880 1957 1901 1926 1982 1902...</td>\n",
       "      <td>[[1796 1829 1907 1895 1879 1848 1889 1909 1928...</td>\n",
       "      <td>1</td>\n",
       "      <td>20.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7.4</td>\n",
       "      <td>16:22:55</td>\n",
       "      <td>[[1904 1872 1881 1887 1910 1907 1908 1897 1908...</td>\n",
       "      <td>[[1561 1703 1787 1777 1702 1820 1749 1847 1893...</td>\n",
       "      <td>1</td>\n",
       "      <td>20.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>7.4</td>\n",
       "      <td>16:22:55</td>\n",
       "      <td>[[1892 1863 1907 1850 1903 1950 1973 1914 1948...</td>\n",
       "      <td>[[2053 1829 1907 1861 1937 1934 1915 1934 1928...</td>\n",
       "      <td>1</td>\n",
       "      <td>20.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7.4</td>\n",
       "      <td>16:22:55</td>\n",
       "      <td>[[1938 1939 1907 1938 1903 1901 1926 1937 1972...</td>\n",
       "      <td>[[1628 1691 1816 1734 1844 1842 1795 1788 1809...</td>\n",
       "      <td>1</td>\n",
       "      <td>20.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7.4</td>\n",
       "      <td>16:22:55</td>\n",
       "      <td>[[1904 1910 1915 1916 1883 1857 1884 1942 1931...</td>\n",
       "      <td>[[1868 1776 1885 1794 1777 1834 1815 1834 1880...</td>\n",
       "      <td>1</td>\n",
       "      <td>20.53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  test_id timestamp  \\\n",
       "0           0      7.4  16:22:55   \n",
       "1           1      7.4  16:22:55   \n",
       "2           2      7.4  16:22:55   \n",
       "3           3      7.4  16:22:55   \n",
       "4           4      7.4  16:22:55   \n",
       "\n",
       "                                         thermal_top  \\\n",
       "0  [[1984 1901 1873 1880 1957 1901 1926 1982 1902...   \n",
       "1  [[1904 1872 1881 1887 1910 1907 1908 1897 1908...   \n",
       "2  [[1892 1863 1907 1850 1903 1950 1973 1914 1948...   \n",
       "3  [[1938 1939 1907 1938 1903 1901 1926 1937 1972...   \n",
       "4  [[1904 1910 1915 1916 1883 1857 1884 1942 1931...   \n",
       "\n",
       "                                         thermal_bot  fall temperature  \n",
       "0  [[1796 1829 1907 1895 1879 1848 1889 1909 1928...     1       20.53  \n",
       "1  [[1561 1703 1787 1777 1702 1820 1749 1847 1893...     1       20.53  \n",
       "2  [[2053 1829 1907 1861 1937 1934 1915 1934 1928...     1       20.53  \n",
       "3  [[1628 1691 1816 1734 1844 1842 1795 1788 1809...     1       20.53  \n",
       "4  [[1868 1776 1885 1794 1777 1834 1815 1834 1880...     1       20.53  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pour chaques frame du dataset, il faut reconstituer chaque enregistrement en concatenent les deux thermal top et thermal bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def generate_image_from_dataset(record, size=(640, 320)):\n",
    "    try:\n",
    "        string_listtop = record[\"thermal_top\"].replace(\"\\n\", \"\").replace(\"  \", \" \").replace(\" \",\",\").replace(\"[[\",\"\").replace(\"]]\",\"\").split(\"],[\")\n",
    "        string_listbot = record[\"thermal_bot\"].replace(\"\\n\", \"\").replace(\"  \", \" \").replace(\" \",\",\").replace(\"[[\",\"\").replace(\"]]\",\"\").split(\"],[\")\n",
    "    except Exception as e:\n",
    "        print(\"Error while parsing the record\")\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "    np_listtop = np.array([np.array([int(x) for x in string.split(\",\")]) for string in string_listtop])\n",
    "    np_listbot = np.array([np.array([int(x) for x in string.split(\",\")]) for string in string_listbot])\n",
    "    np_image = np.concatenate((np_listtop, np_listbot), axis=0)\n",
    "\n",
    "    # Normalisation\n",
    "    min_val = np.min(np_image)\n",
    "    max_val = np.max(np_image)\n",
    "\n",
    "    try:\n",
    "        np_image_normalized = ((np_image - min_val) / (max_val - min_val) * 255).astype(np.uint8)\n",
    "    except Exception as e:\n",
    "        print(\"Error while normalizing the image\")\n",
    "        print(e)\n",
    "        return None\n",
    "    \n",
    "    # Redimensionner l'image \n",
    "    resized_image = cv2.resize(np_image_normalized, size, interpolation=cv2.INTER_NEAREST).astype('uint8')\n",
    "    return resized_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "def generate_tensor_from_dataset(record):\n",
    "    string_listtop = record[\"thermal_top\"].replace(\"\\n\", \"\").replace(\"  \", \" \").replace(\" \",\",\").replace(\"[[\",\"\").replace(\"]]\",\"\").split(\"],[\")\n",
    "    string_listbot = record[\"thermal_bot\"].replace(\"\\n\", \"\").replace(\"  \", \" \").replace(\" \",\",\").replace(\"[[\",\"\").replace(\"]]\",\"\").split(\"],[\")\n",
    "\n",
    "    np_listtop = np.array([np.array([int(x) for x in string.split(\",\")]) for string in string_listtop])\n",
    "    np_listbot = np.array([np.array([int(x) for x in string.split(\",\")]) for string in string_listbot])\n",
    "    np_image = np.concatenate((np_listtop, np_listbot), axis=0)\n",
    "    frame_tensor = torch.tensor(np_image)\n",
    "    return frame_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ffmpeg-python in c:\\users\\mcouv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.2.0)\n",
      "Requirement already satisfied: future in c:\\users\\mcouv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ffmpeg-python) (0.18.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ffmpeg-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1621679558.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[9], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    ffmpeg -i ./classes/fall/prout.mp4 -vcodec libx264 classes/fall/prout.mp4\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "ffmpeg -i classes/fall/prout.mp4 -vcodec libx264 classes/fall/prout.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "def convert_videos():\n",
    "    # Attendre pour s'assurer que la vidéo est entièrement écriture\n",
    "    command = f\"wsl bash ./convert_videos.sh\"\n",
    "    # Exécuter la commande avec subprocess\n",
    "    res = os.system(command)\n",
    "    # Vous pouvez également vérifier si la commande s'est terminée avec succès\n",
    "    if res == 0:\n",
    "        print(f\"Conversion completed successfully!\")\n",
    "    else:\n",
    "        print(f\"Error while converting the videos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "\n",
    "def save_video(images_stack, isfall):\n",
    "    unique_id = str(uuid.uuid4()).replace(\"-\", \"\")\n",
    "    # Create and save video\n",
    "    save_path = \"./classes/fall\" if isfall == 1 else \"./classes/no_fall\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    full_path = f\"{save_path}/{unique_id}.mp4\"\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'MP4V') \n",
    "    out = cv2.VideoWriter(full_path, fourcc, 10.0, (640, 320), isColor=False)\n",
    "    for image in images_stack:\n",
    "        out.write(image)\n",
    "    out.release()\n",
    "\n",
    "def generate_videos_from_dataset(records):\n",
    "    current = None\n",
    "    count_test = 0\n",
    "    count_frame = 0\n",
    "    video_images = []\n",
    "\n",
    "    for record in records.iterrows():\n",
    "    \n",
    "        #first test we setup the current test and save the first image\n",
    "        if current is None:\n",
    "            current = record[1]['test_id']\n",
    "            isfall = record[1]['fall']\n",
    "            print(f\"new test : {current}\")\n",
    "            count_test = 1\n",
    "            try :\n",
    "                resized_image = generate_image_from_dataset(record[1])\n",
    "                video_images.append(resized_image)\n",
    "                count_frame+=1\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(\"Error while generating the image\")\n",
    "                print(e)\n",
    "                continue\n",
    "            \n",
    "        #If test is different we save the video and reset the counter\n",
    "        if record[1]['test_id'] != current:\n",
    "            #save the last test\n",
    "            print(f\"Test ended with {count_frame} frames\")\n",
    "            save_video(video_images, isfall)\n",
    "            print(f\"new test : {current}\")\n",
    "            video_images = []\n",
    "            current = record[1]['test_id']\n",
    "            count_test += 1\n",
    "            count_frame = 0\n",
    "            isfall = record[1]['fall']\n",
    "\n",
    "        #save the image\n",
    "        try :\n",
    "            resized_image = generate_image_from_dataset(record[1])\n",
    "            video_images.append(resized_image)\n",
    "            count_frame+=1\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(\"Error while generating the image\")\n",
    "            print(e)\n",
    "            continue\n",
    "        if count_test == 10:\n",
    "            break\n",
    "\n",
    "    #save the last test\n",
    "    print(f\"Test ended with {count_frame} frames\")\n",
    "    save_video(video_images, isfall)\n",
    "    convert_videos()\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision.io import read_video\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import cv2\n",
    "\n",
    "def load_video_with_opencv(video_path):\n",
    "    vid = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = vid.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Convertir en niveaux de gris (même si c'est déjà le cas)\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        frames.append(torch.tensor(gray_frame))\n",
    "    vid.release()\n",
    "    video_tensor = torch.stack(frames)\n",
    "    return video_tensor.unsqueeze(1)  # Ajoute un canal\n",
    "\n",
    "\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - root_dir (string): Dossier avec toutes les vidéos.\n",
    "        - transform (callable, optional): Transformation optionnelle à appliquer\n",
    "            sur une vidéo.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.classes = os.listdir(root_dir)\n",
    "        self.classes.sort()\n",
    "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "\n",
    "        self.video_list = []\n",
    "        for cls_name in self.classes:\n",
    "            cls_dir = os.path.join(root_dir, cls_name)\n",
    "            for video_name in os.listdir(cls_dir):\n",
    "                self.video_list.append((os.path.join(cls_dir, video_name), self.class_to_idx[cls_name]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path, label = self.video_list[idx]\n",
    "        \n",
    "        # Charger la vidéo. Pour simplifier, nous chargerons seulement les images ici.\n",
    "        # Pour un cas d'utilisation réel, vous pourriez vouloir travailler avec \n",
    "        # les images et l'audio.\n",
    "        video = load_video_with_opencv(video_path)\n",
    "        \n",
    "        if self.transform:\n",
    "            video = self.transform(video)\n",
    "\n",
    "        return video, label\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import cv2\n",
    "from torch.utils.data import random_split\n",
    "import random\n",
    "from torch.utils.data import Subset\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def uniform_frame_sampling(video_tensor, target_frames=90):\n",
    "    \"\"\"\n",
    "    Sélectionne un nombre uniformément distribué de frames pour que toutes les vidéos aient la même taille.\n",
    "\n",
    "    Args:\n",
    "        video_tensor (torch.Tensor): La vidéo originale de forme (T, C, H, W)\n",
    "        target_frames (int): Nombre de frames cibles à obtenir\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Vidéo échantillonnée de forme (target_frames, C, H, W)\n",
    "    \"\"\"\n",
    "\n",
    "    # Obtenir le nombre actuel de frames\n",
    "    current_frames = video_tensor.shape[0]\n",
    "\n",
    "    # Si la vidéo actuelle a exactement le nombre cible de frames, la renvoyer telle quelle\n",
    "    if current_frames == target_frames:\n",
    "        return video_tensor\n",
    "\n",
    "    # Calculer les indices des frames à échantillonner\n",
    "    indices = torch.linspace(0, current_frames - 1, target_frames).long()\n",
    "\n",
    "    return video_tensor[indices]\n",
    "\n",
    "# Pour l'utiliser dans le VideoDataset :\n",
    "def video_resize_and_sample(video_tensor, size=(16, 8), T=90):\n",
    "    # Resize\n",
    "    resized_video = []\n",
    "    for frame in video_tensor:\n",
    "        # S'assurer que la frame est en niveaux de gris (HxW) et non (1xHxW)\n",
    "        frame = frame.squeeze(0)\n",
    "        frame = frame.numpy()\n",
    "        resized_frame = cv2.resize(frame, size, interpolation=cv2.INTER_NEAREST)\n",
    "        resized_video.append(torch.tensor(resized_frame).unsqueeze(0))  # Ajouter de nouveau le canal\n",
    "    \n",
    "    resized_video_tensor = torch.stack(resized_video)\n",
    "    # Uniform sampling\n",
    "    sampled_video_tensor = uniform_frame_sampling(resized_video_tensor, T)\n",
    "     # Normalisation entre [-1, 1]\n",
    "    normalized_tensor = (sampled_video_tensor / 127.5)\n",
    "\n",
    "    return normalized_tensor.to(device)\n",
    "\n",
    "# Utilisation\n",
    "transform = video_resize_and_sample\n",
    "video_dataset = VideoDataset(root_dir='./classes/', transform=transform)\n",
    "\n",
    "\n",
    "train_indices = []\n",
    "test_indices = []\n",
    "\n",
    "# Pour chaque classe, récupérez les indices de ses vidéos.\n",
    "for cls_name in video_dataset.classes:\n",
    "    cls_indices = [i for i, (path, label) in enumerate(video_dataset.video_list) if label == video_dataset.class_to_idx[cls_name]]\n",
    "    \n",
    "    # Mélangez ces indices.\n",
    "    random.shuffle(cls_indices)\n",
    "    \n",
    "    # Séparez-les en fonction des ratios d'entraînement et de test.\n",
    "    cls_train_size = int(0.8 * len(cls_indices))\n",
    "    train_indices.extend(cls_indices[:cls_train_size])\n",
    "    test_indices.extend(cls_indices[cls_train_size:])\n",
    "\n",
    "# Créez des sous-ensembles d'entraînement et de test en utilisant ces indices.\n",
    "train_dataset = Subset(video_dataset, train_indices)\n",
    "test_dataset = Subset(video_dataset, test_indices)\n",
    "\n",
    "# DataLoader pour chaque ensemble:\n",
    "train_loader = DataLoader(train_dataset, batch_size=20, num_workers=1,shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Test\n",
    "# count = 0\n",
    "# for video_batch, labels in train_loader:\n",
    "#     video_batch, labels = video_batch.to(device), labels.to(device)\n",
    "#     print(f\"batch shape = {video_batch.shape}, device = {video_batch.device}\")\n",
    "#     print(f\"labels shape = {labels.shape}, device = {labels.device}\")\n",
    "#     count += 1\n",
    "#     if count == 1:\n",
    "#         image = video_batch[0][89]\n",
    "#         print(image.shape)\n",
    "#         print(image)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorchvideoNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading pytorchvideo-0.1.5.tar.gz (132 kB)\n",
      "     ---------------------------------------- 0.0/132.7 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/132.7 kB ? eta -:--:--\n",
      "     ----------- ------------------------- 41.0/132.7 kB 495.5 kB/s eta 0:00:01\n",
      "     -------------------------------------- 132.7/132.7 kB 1.3 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting fvcore (from pytorchvideo)\n",
      "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
      "     ---------------------------------------- 0.0/50.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 50.2/50.2 kB ? eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: av in c:\\users\\mcouv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pytorchvideo) (10.0.0)\n",
      "Collecting parameterized (from pytorchvideo)\n",
      "  Downloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
      "Collecting iopath (from pytorchvideo)\n",
      "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
      "     ---------------------------------------- 0.0/42.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 42.2/42.2 kB ? eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: networkx in c:\\users\\mcouv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pytorchvideo) (3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\mcouv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from fvcore->pytorchvideo) (1.25.2)\n",
      "Collecting yacs>=0.1.6 (from fvcore->pytorchvideo)\n",
      "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mcouv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from fvcore->pytorchvideo) (6.0.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mcouv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from fvcore->pytorchvideo) (4.66.1)\n",
      "Collecting termcolor>=1.1 (from fvcore->pytorchvideo)\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Requirement already satisfied: Pillow in c:\\users\\mcouv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from fvcore->pytorchvideo) (10.0.0)\n",
      "Collecting tabulate (from fvcore->pytorchvideo)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\mcouv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from iopath->pytorchvideo) (4.7.1)\n",
      "Collecting portalocker (from iopath->pytorchvideo)\n",
      "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\mcouv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from portalocker->iopath->pytorchvideo) (306)\n",
      "Requirement already satisfied: colorama in c:\\users\\mcouv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm->fvcore->pytorchvideo) (0.4.6)\n",
      "Building wheels for collected packages: pytorchvideo, fvcore, iopath\n",
      "  Building wheel for pytorchvideo (pyproject.toml): started\n",
      "  Building wheel for pytorchvideo (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pytorchvideo: filename=pytorchvideo-0.1.5-py3-none-any.whl size=188698 sha256=c9db7e3ef3112aedd6cba6e4595586d73af60e54fff5b4b2c3bdf82b1a7d4ded\n",
      "  Stored in directory: c:\\users\\mcouv\\appdata\\local\\pip\\cache\\wheels\\a4\\6d\\ae\\d016375a73be141a0e11bb42289e2d0b046c35687fc8010ecc\n",
      "  Building wheel for fvcore (pyproject.toml): started\n",
      "  Building wheel for fvcore (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61411 sha256=1be490d5ad81b9470dd46d99d58fd3a19047919870cb5c26f959ac75e27f8f47\n",
      "  Stored in directory: c:\\users\\mcouv\\appdata\\local\\pip\\cache\\wheels\\65\\71\\95\\3b8fde5c65c6e4a806e0867c1651dcc71a1cb2f3430e8f355f\n",
      "  Building wheel for iopath (pyproject.toml): started\n",
      "  Building wheel for iopath (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31541 sha256=4dbd79e678e137c5e71749fb934e87efbec96c16a57a5f1d933bd0a6f3b0016d\n",
      "  Stored in directory: c:\\users\\mcouv\\appdata\\local\\pip\\cache\\wheels\\ba\\5e\\16\\6117f8fe7e9c0c161a795e10d94645ebcf301ccbd01f66d8ec\n",
      "Successfully built pytorchvideo fvcore iopath\n",
      "Installing collected packages: yacs, termcolor, tabulate, portalocker, parameterized, iopath, fvcore, pytorchvideo\n",
      "Successfully installed fvcore-0.1.5.post20221221 iopath-0.1.10 parameterized-0.9.0 portalocker-2.7.0 pytorchvideo-0.1.5 tabulate-0.9.0 termcolor-2.3.0 yacs-0.1.8\n"
     ]
    }
   ],
   "source": [
    "pip install pytorchvideo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilisation du device cuda\n",
      "Epoch 1/10, Loss: 0.6931472420692444\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 47\u001b[0m\n\u001b[0;32m     43\u001b[0m num_epochs \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m  \u001b[39m# ou tout autre nombre que vous jugez approprié\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m---> 47\u001b[0m     \u001b[39mfor\u001b[39;00m video_batch, labels \u001b[39min\u001b[39;00m train_loader:\n\u001b[0;32m     48\u001b[0m         video_batch, labels \u001b[39m=\u001b[39m video_batch\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     49\u001b[0m         \u001b[39m# Permuter les dimensions T et C\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;49;00m idx \u001b[39min\u001b[39;49;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataset.py:298\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[0;32m    297\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[1;32m--> 298\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
      "Cell \u001b[1;32mIn[11], line 53\u001b[0m, in \u001b[0;36mVideoDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     48\u001b[0m video_path, label \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvideo_list[idx]\n\u001b[0;32m     50\u001b[0m \u001b[39m# Charger la vidéo. Pour simplifier, nous chargerons seulement les images ici.\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[39m# Pour un cas d'utilisation réel, vous pourriez vouloir travailler avec \u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[39m# les images et l'audio.\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m video \u001b[39m=\u001b[39m load_video_with_opencv(video_path)\n\u001b[0;32m     55\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform:\n\u001b[0;32m     56\u001b[0m     video \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(video)\n",
      "Cell \u001b[1;32mIn[11], line 12\u001b[0m, in \u001b[0;36mload_video_with_opencv\u001b[1;34m(video_path)\u001b[0m\n\u001b[0;32m     10\u001b[0m frames \u001b[39m=\u001b[39m []\n\u001b[0;32m     11\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m---> 12\u001b[0m     ret, frame \u001b[39m=\u001b[39m vid\u001b[39m.\u001b[39;49mread()\n\u001b[0;32m     13\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m ret:\n\u001b[0;32m     14\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class VideoClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VideoClassifier, self).__init__()\n",
    "        \n",
    "        # La couche d'entrée attend des vidéos de la forme (C, T, H, W) = (1, 90, 8, 16)\n",
    "        self.conv_layer1 = nn.Sequential(\n",
    "            nn.Conv3d(1, 32, kernel_size=(3, 3, 3), padding=1),  # 3D convolution\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2))\n",
    "        )\n",
    "        \n",
    "        self.conv_layer2 = nn.Sequential(\n",
    "            nn.Conv3d(32, 64, kernel_size=(3, 3, 3), padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2))\n",
    "        )\n",
    "        \n",
    "        # Ici, on aplatit la sortie pour la connecter à des couches entièrement connectées\n",
    "        self.fc1 = nn.Linear(64 * 22 * 2 * 4, 512)  # La taille dépend de la sortie du dernier MaxPool3d\n",
    "        self.fc2 = nn.Linear(512, 1)  # 1 neurone en sortie pour une classification binaire\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer1(x)\n",
    "        x = self.conv_layer2(x)\n",
    "        x = x.view(x.size(0), -1)  # flatten\n",
    "        x = nn.ReLU()(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x.squeeze()  # la sortie est de forme (batch_size)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Utilisation du device {device}\")\n",
    "\n",
    "model = VideoClassifier()\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "num_epochs = 10  # ou tout autre nombre que vous jugez approprié\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for video_batch, labels in train_loader:\n",
    "        video_batch, labels = video_batch.to(device), labels.to(device)\n",
    "        # Permuter les dimensions T et C\n",
    "        video_batch = video_batch.permute(0, 2, 1, 3, 4)\n",
    "        labels = labels.float()  # Convertir les labels en float pour la BCEWithLogitsLoss\n",
    "        \n",
    "        # Forward\n",
    "        outputs = model(video_batch)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pytorch_lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1984, 1901, 1873, 1880, 1957, 1901, 1926, 1982, 1902, 1977, 1938,\n",
       "        2003, 1970, 1982, 1950, 2087],\n",
       "       [1918, 1888, 1925, 1922, 1888, 1929, 1956, 1998, 1989, 1969, 1935,\n",
       "        1958, 1952, 1966, 2003, 1921],\n",
       "       [1989, 1887, 1903, 1951, 1964, 1928, 1941, 1944, 1967, 1939, 1993,\n",
       "        2383, 2157, 2014, 2036, 1961],\n",
       "       [1865, 1861, 1789, 1844, 1955, 1887, 1922, 1918, 1952, 1883, 1938,\n",
       "        2483, 2429, 2026, 1943, 1944]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
